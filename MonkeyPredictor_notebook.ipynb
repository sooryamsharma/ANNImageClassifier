{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "e37d66080a35ccd885ab05f83da56fb3",
          "grade": false,
          "grade_id": "cell-3c8e604f2bd13b29",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "HIQKzVncMcJu",
        "colab_type": "text"
      },
      "source": [
        "# Notebook Description\n",
        "The purpose of this notebook is to gain some experience with multilayer and convolutional networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x85-nXaWb9rs",
        "colab_type": "text"
      },
      "source": [
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "cec65fba906620ef1c980ccb6ff4d93d",
          "grade": false,
          "grade_id": "cell-8d4b87ee874eb380",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "LE7oaWt7McJv",
        "colab_type": "text"
      },
      "source": [
        "# Part 1 - Fully Connected Feed Forward Networks\n",
        "Training a feedforward networks in python to perform classification of 10 different species of Monkeys.<br>\n",
        "\n",
        "The dataset consists of two files, training and validation. Each folder contains 10 subfolders labeled as n0~n9, each corresponding a species form Wikipedia's monkey cladogram. Images are 400x300 px **or larger** and JPEG format (almost 1400 images). Images were downloaded with help of the googliser open source code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsJ5uC65WntU",
        "colab_type": "text"
      },
      "source": [
        "### We need to install Augmentor for image data augmentation, Augmentor is much better than Keras image augmentation function.\n",
        "\n",
        "We will be using Augmentor class for image Augmentation. <br>\n",
        "\n",
        "We'll be resizing every image to 150\\*150\\*3, to reduce the computational cost of the network, but that will affect the model very badly. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onw_OCrkNIvz",
        "colab_type": "code",
        "outputId": "217b824b-bec9-4b5e-df23-ea02a256195f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "pip install Augmentor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Augmentor\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/f4/b0eaa9d3b4120a5450ac92d4417907ca60fad5749c1f50ed95f720792350/Augmentor-0.2.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (0.16.0)\n",
            "Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (4.28.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (1.16.3)\n",
            "Requirement already satisfied: Pillow>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=4.0.0->Augmentor) (0.46)\n",
            "Installing collected packages: Augmentor\n",
            "Successfully installed Augmentor-0.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "885a76e82190cb88b89d80a7d2880264",
          "grade": true,
          "grade_id": "1a",
          "locked": false,
          "points": 8,
          "schema_version": 1,
          "solution": true
        },
        "id": "YymD61D1McJx",
        "colab_type": "code",
        "outputId": "f2ea6dd7-2eca-4201-f4a9-eba59ee252f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import math\n",
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import scipy\n",
        "import Augmentor # <------------- you may need to install this\n",
        "import random\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from skimage.io import imread \n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "from glob import glob\n",
        "np.random.seed(1)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'Colab Notebooks/ANNMonkeySpeciesClassification/'\n",
        "dirpath = base_dir\n",
        "\n",
        "# Desired Height and Width of the image because the data is large-------------------------------------------------------------------------\n",
        "hght = 150\n",
        "wdth = 150\n",
        "\n",
        "\n",
        "# Data Augmentation, very necesarry because we are implementing Fully connected NN architecture on Image Data, instead of CNN.------------\n",
        "def augmentation(df, drctry):\n",
        "    for i, rows in df.iterrows():\n",
        "        path = drctry+rows.label.strip()\n",
        "        p = Augmentor.Pipeline(path)\n",
        "        # Applying image augmentation\n",
        "        p.resize(probability=1, width=wdth, height=hght)\n",
        "        p.rotate(probability=1, max_left_rotation=15, max_right_rotation=15)\n",
        "        p.flip_left_right(probability=0.5)\n",
        "        # no of sample you want to generate\n",
        "        p.process()\n",
        "\n",
        "# After augmentation we need to replace the images back into the directory.----------------------------------------------------------------\n",
        "def rename(df, drctry):\n",
        "    for i, rows in df.iterrows():\n",
        "        lbl = rows.label.strip()\n",
        "        origin = drctry+lbl+\"/output/\"\n",
        "        destination = drctry+lbl+\"/\"\n",
        "        #deleting existing files\n",
        "        flst = glob(os.path.join(destination, \"*.jpg\"))\n",
        "        for f in flst:\n",
        "            os.remove(f)\n",
        "        for i, filename in enumerate(os.listdir(origin)):\n",
        "            os.rename(origin + filename, destination + lbl + str(i).zfill(3) + \".jpg\")\n",
        "        os.rmdir(drctry+lbl+\"/output\")\n",
        "        \n",
        "# defining one-hot-matrix definition------------------------------------------------------------------------------------------------------\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n",
        "\n",
        "# Defining directory paths-----------------------------------------------------------------------------------------------------------------\n",
        "labels_path = dirpath+r'/monkey_labels.txt'\n",
        "train_dir = dirpath+'/training/'\n",
        "test_dir = dirpath+'/validation/'\n",
        "\n",
        "# Reading monkey_labels.txt using Pandas---------------------------------------------------------------------------------------------------\n",
        "cols = ['label','latinName', 'commonName','training_images', 'validation_images']\n",
        "labels = pd.read_csv(labels_path, names=cols, sep=',', skiprows=1)\n",
        "\n",
        "# Applying Data Augmentation (comment once done)-------------------------------------------------------------------------------------------\n",
        "#augmentation(labels, train_dir)\n",
        "#augmentation(labels, test_dir)\n",
        "#rename(labels, train_dir)\n",
        "#rename(labels, test_dir)\n",
        "\n",
        "# Creating training and validation data frames---------------------------------------------------------------------------------------------\n",
        "clmns =  ['path']\n",
        "df_train  = pd.DataFrame(columns = clmns)\n",
        "for idx, row in labels.iterrows():\n",
        "    path = train_dir+labels.label[idx].strip()+'/'\n",
        "    df_temp = pd.DataFrame({'path': glob(os.path.join(path, '*.jpg'))})\n",
        "    df_train = df_train.append(df_temp, ignore_index=True)\n",
        "df_train['id'] = df_train.path.map(lambda x: x.rsplit(\"/\",1)[1].split(\".\")[0])\n",
        "df_train['label'] = df_train.path.map(lambda x: x.rsplit(\"/\",1)[1].split(\".\")[0][:2])\n",
        "#df_train.head(10)\n",
        "\n",
        "col_names =  ['path']\n",
        "df_test  = pd.DataFrame(columns = col_names)\n",
        "for idx, row in labels.iterrows():\n",
        "    path = test_dir+labels.label[idx].strip()+'/'\n",
        "    df_temp = pd.DataFrame({'path': glob(os.path.join(path, '*.jpg'))})\n",
        "    df_test = df_test.append(df_temp, ignore_index=True)\n",
        "df_test['id'] = df_test.path.map(lambda x: x.rsplit(\"/\",1)[1].split(\".\")[0])\n",
        "df_test['label'] = df_test.path.map(lambda x: x.rsplit(\"/\",1)[1].split(\".\")[0][:2])\n",
        "#df_test.head(10)\n",
        "\n",
        "# Creating training and validation data---------------------------------------------------------------------------------------------------\n",
        "global train_x, test_x\n",
        "train_x = np.empty((1, hght*wdth*3)) # since the size of one image, after flattening would be is height*width*3\n",
        "test_x = np.empty((1, hght*wdth*3))\n",
        "\n",
        "for idx, row in df_train.iterrows():\n",
        "    image = cv2.imread(df_train.path[idx], -1)\n",
        "    image_flatten = np.reshape(image, [-1])\n",
        "    image_flatten = image_flatten/255.\n",
        "    train_x = np.vstack((train_x, image_flatten))    \n",
        "train_x = train_x[1:]\n",
        "train_x = train_x.T\n",
        "#print(train_x.shape)\n",
        "\n",
        "for idx, row in df_test.iterrows():\n",
        "    image = cv2.imread(df_test.path[idx], -1)\n",
        "    image_flatten = np.reshape(image, [-1])\n",
        "    image_flatten = image_flatten/255.\n",
        "    test_x = np.vstack((test_x, image_flatten))\n",
        "test_x = test_x[1:]\n",
        "test_x = test_x.T\n",
        "#print(test_x.shape)\n",
        "\n",
        "# Creating training and validation labels---------------------------------------------------------------------------------------------------\n",
        "global train_y, test_y\n",
        "\n",
        "train_y_orig = np.array(df_train.label)\n",
        "train_y = np.empty((int(train_x.shape[1])), dtype=np.int8)\n",
        "labels = np.empty((1), dtype=np.int8)\n",
        "for label in train_y_orig:\n",
        "    label = label[1]\n",
        "    labels = np.append(labels, int(label))\n",
        "labels = labels[1:]\n",
        "train_y = np.vstack((train_y, labels))\n",
        "train_y = train_y[1:]\n",
        "train_y = convert_to_one_hot(train_y, 10)\n",
        "#print(train_y.shape)\n",
        "\n",
        "test_y_orig = np.array(df_test.label)\n",
        "test_y = np.empty((int(test_x.shape[1])), dtype=np.int8)\n",
        "labels = np.empty((1), dtype=np.int8)\n",
        "for label in test_y_orig:\n",
        "    label = label[1]\n",
        "    labels = np.append(labels, int(label,10))\n",
        "labels = labels[1:]\n",
        "test_y = np.vstack((test_y, labels))\n",
        "test_y = test_y[1:]\n",
        "test_y = convert_to_one_hot(test_y, 10)\n",
        "#print(test_y.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "118d140bee859fbcecc470d176820157",
          "grade": false,
          "grade_id": "cell-8be2b07e3ef21eeb",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "YX6KLdYGMcJ0",
        "colab_type": "text"
      },
      "source": [
        "# 1A. Model definition\n",
        "Creating a fully connected feedforward network model of 6 layers.\n",
        "\n",
        "fc1 : [2048, hght \\* wght \\* 3]\n",
        "\n",
        "fc2 : [1024, 2048] \n",
        "\n",
        "fc3 : [512, 1024]\n",
        "\n",
        "fc4 : [512, 512]\n",
        "\n",
        "fc5 : [64, 512]\n",
        "\n",
        "fc6 : [10, 64]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "8ac54db7042f6d89e62729e78534b54a",
          "grade": true,
          "grade_id": "1b",
          "locked": false,
          "points": 8,
          "schema_version": 1,
          "solution": true
        },
        "id": "aIlebgljMcJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Definition ----------------------------------------------------------------------------------------------------------------------\n",
        "'''\n",
        "ANN architecture for the given dataset\n",
        "\n",
        "fc1 : [2048, hght*wght*3]\n",
        "fc2 : [1024, 2048] \n",
        "fc3 : [512, 1024]\n",
        "fc4 : [512, 512]\n",
        "fc5 : [64, 512]\n",
        "fc6 : [10, 64]\n",
        "\n",
        "'''\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------\n",
        "def create_placeholders(n_x, n_y):\n",
        "#    Creates the placeholders for the tensorflow session.    \n",
        "#    Arguments:\n",
        "#    n_x -- scalar, size of an image vector (num_px * num_px = 300 * 300 * 3 = 270000)\n",
        "#    n_y -- scalar, number of classes (from 0 to 9, so -> 10)    \n",
        "#    Returns:\n",
        "#    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
        "#    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
        "\n",
        "    X = tf.placeholder(tf.float32, [n_x, None])\n",
        "    Y = tf.placeholder(tf.float32, [n_y, None])\n",
        "    \n",
        "    return X, Y\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------\n",
        "#Definition of neural nets------------------------------------------------------------------------------------------------------------\n",
        "def forward_propagation(X, eps_regularization):\n",
        "    \n",
        "    X = tf.transpose(X)\n",
        "    \n",
        "    #First fully connected layer.\n",
        "    fc1 = tf.contrib.layers.fully_connected(X, num_outputs=2048, activation_fn = tf.nn.leaky_relu, \n",
        "                        weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
        "                        biases_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                        weights_regularizer=tf.contrib.layers.l2_regularizer(eps_regularization))\n",
        "    #Dropout.\n",
        "    fc1 = tf.nn.dropout(fc1, rate=0.5)\n",
        "    \n",
        "    #Second fully connected layer.\n",
        "    fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024, activation_fn = tf.nn.leaky_relu, \n",
        "                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
        "                         biases_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                         weights_regularizer=tf.contrib.layers.l2_regularizer(eps_regularization))\n",
        "    #Dropout.\n",
        "    fc2 = tf.nn.dropout(fc2, rate=0.3)\n",
        "    \n",
        "    #Third fully connected layer.\n",
        "    fc3 = tf.contrib.layers.fully_connected(fc2, num_outputs=512, activation_fn = tf.nn.leaky_relu, \n",
        "                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
        "                         biases_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                         weights_regularizer=tf.contrib.layers.l2_regularizer(eps_regularization))\n",
        "    #Dropout.\n",
        "    fc3 = tf.nn.dropout(fc3, rate=0.1)\n",
        "    \n",
        "    #Fourth fully connected layer.\n",
        "    fc4 = tf.contrib.layers.fully_connected(fc3, num_outputs=512, activation_fn = tf.nn.leaky_relu, \n",
        "                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
        "                         biases_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                         weights_regularizer=tf.contrib.layers.l2_regularizer(eps_regularization))\n",
        "    \n",
        "    #Fifth fully connected layer.\n",
        "    fc5 = tf.contrib.layers.fully_connected(fc4, num_outputs=64, activation_fn = tf.nn.leaky_relu, \n",
        "                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
        "                         biases_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                         weights_regularizer=tf.contrib.layers.l2_regularizer(eps_regularization))\n",
        "    \n",
        "    #Output layer.\n",
        "    output_layer = tf.contrib.layers.fully_connected(fc5, num_outputs=10)\n",
        "    \n",
        "    output_layer = tf.transpose(output_layer)\n",
        "    \n",
        "    return output_layer\n",
        "  \n",
        "#----------------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_cost(output_layer, Y):\n",
        "    \"\"\"\n",
        "    Computes the cost    \n",
        "    Arguments:\n",
        "    output_layer -- output of forward propagation (output of the last LINEAR unit), of shape (10, number of examples)\n",
        "    Y -- \"true\" labels vector placeholder, same shape as output_layer\n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    \"\"\"\n",
        "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
        "    logits = tf.transpose(output_layer)\n",
        "    labels = tf.transpose(Y)\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels)\n",
        "    cost = tf.reduce_mean(cross_entropy)\n",
        "    return cost\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------\n",
        "def random_mini_batches(X, Y, mini_batch_size, seed):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector, of shape (1, number of examples)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "55eb0ede53799995cf4035dacce2377c",
          "grade": false,
          "grade_id": "cell-b69989d6191cfae0",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "uwreofGAMcJ4",
        "colab_type": "text"
      },
      "source": [
        "# 1B - Training\n",
        "Training a single fully connected network for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "fb5d9e04255f5cfdfe2860f473dfc465",
          "grade": true,
          "grade_id": "1c",
          "locked": false,
          "points": 8,
          "schema_version": 1,
          "solution": true
        },
        "id": "1UJbX91ZMcJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters---------------------------------------------------------------------------------------------------------------------\n",
        "nEpochs = 1500\n",
        "miniBatchSize = 16\n",
        "learningRate = 0.001\n",
        "\n",
        "\n",
        "# Model definition---------------------------------------------------------------------------------------------------------------------\n",
        "def model(X_train, Y_train, X_test, Y_test, num_epochs = nEpochs, minibatch_size = miniBatchSize, learning_rate = learningRate, print_cost = True):\n",
        "    \"\"\"\n",
        "    Implements a six-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
        "    Arguments:\n",
        "    X_train -- training set, of shape (input size = 67500, number of training examples = 1097)\n",
        "    Y_train -- test set, of shape (output size = 10, number of training examples = 1097)\n",
        "    X_test -- training set, of shape (input size = 67500, number of training examples = 272)\n",
        "    Y_test -- test set, of shape (output size = 10, number of test examples = 272)\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_epochs -- number of epochs of the optimization loop\n",
        "    minibatch_size -- size of a minibatch\n",
        "    print_cost -- True to print the cost every 100 epochs\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
        "    tf.set_random_seed(1)                             # to keep consistent results\n",
        "    seed = 3                                          # to keep consistent results\n",
        "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
        "    n_y = Y_train.shape[0]                            # n_y : output size\n",
        "    costs = []                                        # To keep track of the cost\n",
        "    \n",
        "    # Create Placeholders of shape (n_x, n_y)\n",
        "    X, Y = create_placeholders(n_x, n_y)\n",
        "    \n",
        "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
        "    output_layer = forward_propagation(X, eps_regularization)\n",
        "    \n",
        "    # Cost function: Add cost function to tensorflow graph\n",
        "    cost = compute_cost(output_layer,Y)\n",
        "    \n",
        "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "    \n",
        "    # Initialize all the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Start the session to compute the tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        # Run the initialization\n",
        "        sess.run(init)\n",
        "        \n",
        "        # Do the training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
        "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "            seed = seed + 1\n",
        "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
        "\n",
        "            for minibatch in minibatches:\n",
        "                # Select a minibatch\n",
        "                (minibatch_X, minibatch_Y) = minibatch\n",
        "                \n",
        "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
        "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
        "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
        "                \n",
        "                epoch_cost += minibatch_cost / num_minibatches\n",
        "\n",
        "            # Print the cost every epoch\n",
        "            if print_cost == True and epoch % 100 == 0:\n",
        "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "            if print_cost == True and epoch % 5 == 0:\n",
        "                costs.append(epoch_cost)\n",
        "                \n",
        "        # plot the cost\n",
        "        plt.plot(np.squeeze(costs))\n",
        "        plt.ylabel('cost')\n",
        "        plt.xlabel('iterations (per tens)')\n",
        "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "        plt.show()\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_prediction = tf.equal(tf.argmax(output_layer), tf.argmax(Y))\n",
        "\n",
        "        # Calculate accuracy on the test set\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
        "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "93628442b441be68b730b1fa9c16bddb",
          "grade": false,
          "grade_id": "cell-c268beb4e04bba08",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "1h63x7QfMcJ8",
        "colab_type": "text"
      },
      "source": [
        "# 1B - Quantitative Evaluation\n",
        "Doing some quantitative evaluation of our trained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "4917b1d7f5cafa5999c0bdfd7b32255b",
          "grade": true,
          "grade_id": "1d",
          "locked": false,
          "points": 8,
          "schema_version": 1,
          "solution": true
        },
        "id": "dPqz6jpqMcJ9",
        "colab_type": "code",
        "outputId": "a46dbfd9-f14c-4b16-fcdd-d76f388a1c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Training starts here------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "model(train_x, train_y, test_x, test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 0: 2.343418\n",
            "Cost after epoch 100: 0.438468\n",
            "Cost after epoch 200: 0.049341\n",
            "Cost after epoch 300: 0.012198\n",
            "Cost after epoch 400: 0.006485\n",
            "Cost after epoch 500: 0.011239\n",
            "Cost after epoch 600: 0.003074\n",
            "Cost after epoch 700: 0.007427\n",
            "Cost after epoch 800: 0.002654\n",
            "Cost after epoch 900: 0.000820\n",
            "Cost after epoch 1000: 0.004080\n",
            "Cost after epoch 1100: 0.000856\n",
            "Cost after epoch 1200: 0.000522\n",
            "Cost after epoch 1300: 0.000987\n",
            "Cost after epoch 1400: 0.003500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4XHd97/H3d0aa0UijxVq8b7Gd\nFWKaxFncBdJSmqW0KZBAaAuBlqYBcrs/vWG5SUovvZRAW2iAEJqQBCgEAm1NmpAGGhJoVickTuzE\niR3seLe8aLN2zff+cc7IY1kjj2yNjqTzeT3PPJo55zdnvkcjzWd+Z/kdc3dEREQAElEXICIiU4dC\nQUREhikURERkmEJBRESGKRRERGSYQkFERIYpFGRGMrP7zeyqqOsQmW4UCjKhzGyLmf161HW4+yXu\nfmfUdQCY2Y/N7AOT8DppM7vdzDrMbLeZ/cUx2v952K4jfF66YN5SM3vIzLrN7KXC99TMXm9mD5jZ\nPjPTiU4zjEJBph0zq4i6hrypVAtwI3AysAT4VeCvzezi0Rqa2UXAdcCbw/bLgL8paPJN4GdAE/Ax\n4B4zawnnDQDfBv5w4ldBoqZQkEljZm81s2fNrM3MHjWzlQXzrjOzzWbWaWYbzOxtBfPeZ2b/Y2b/\naGb7gRvDaT81s8+Y2UEz+7mZXVLwnOFv5yW0PcnMHglf+4dm9gUz+3qRdbjQzLab2f82s93AV81s\nlpnda2at4fLvNbOFYftPAr8C3GxmXWZ2czj9NDN70MwOmNlGM3vnBPyKrwL+1t0PuvuLwFeA943R\n9jZ3X+/uB4G/zbc1s1OAs4Eb3L3H3b8LPA+8A8DdN7r7bcD6CahZphiFgkwKMzsLuB34Y4Jvn18G\n1hRssthM8OFZT/CN9etmNq9gEecDrwJzgE8WTNsINAOfBm4zMytSwlht/xV4MqzrRuA9x1iduUAj\nwTfsqwn+j74aPl4M9AA3A7j7x4CfANe6e9bdrzWzGuDB8HVnA1cCXzSzM0Z7MTP7Yhiko93WhW1m\nAfOA5wqe+hzwuiLr8LpR2s4xs6Zw3qvu3lnismQGUSjIZLka+LK7P+HuQ+H2/j7gAgB3/46773T3\nnLvfDbwCnFfw/J3u/s/uPujuPeG0re7+FXcfAu4k+FCcU+T1R21rZouBc4Hr3b3f3X8KrDnGuuQI\nvkX3hd+k97v7d929O/wg/STwpjGe/1Zgi7t/NVyfnwHfBa4YrbG7f8jdG4rc8r2tbPizveCp7UBt\nkRqyo7QlbD9y3rGWJTOIQkEmyxLgLwu/5QKLgPkAZvbegk1LbcDrCb7V520bZZm783fcvTu8mx2l\n3Vht5wMHCqYVe61Cre7em39gZtVm9mUz22pmHcAjQIOZJYs8fwlw/ojfxe8R9ECOV1f4s65gWh3Q\nOUrbfPuRbQnbj5x3rGXJDKJQkMmyDfjkiG+51e7+TTNbQrD9+1qgyd0bgBeAwk1B5TrKZRfQaGbV\nBdMWHeM5I2v5S+BU4Hx3rwPeGE63Iu23AQ+P+F1k3f2Do72Ymd0S7o8Y7bYeINwvsAt4Q8FT30Dx\n7f7rR2m7x933h/OWmVntiPnahxADCgUph0ozqyq4VRB86F9jZudboMbMfjP84Kkh+OBsBTCz9xP0\nFMrO3bcCawl2XqfMbDXwW+NcTC3BfoQ2M2sEbhgxfw/B0T159wKnmNl7zKwyvJ1rZqcXqfGaMDRG\nuxVu578L+Hi44/s04I+AO4rUfBfwh2Z2hpk1AB/Pt3X3l4FngRvC9+9twEqCTVyE718VkAofVxXs\nG5JpTqEg5XAfwYdk/naju68l+JC6GTgIbCI82sXdNwCfBR4j+AA9E/ifSaz394DVwH7g/wJ3E+zv\nKNU/ARlgH/A48IMR8z8HXB4emfT5cL/DbxDsYN5JsGnr74ET/WC9gWCH/VbgYeAmd/8BgJktDnsW\niwHC6Z8GHgJeC59TGGZXAqsI3qtPAZe7e2s4bwnB+5rvOfQQ7MSXGcB0kR2RI5nZ3cBL7j7yG7/I\njKeegsReuOlmuZklLDjZ6zLg36OuSyQKU+lsTJGozAW+R3Cewnbgg+FhoiKxo81HIiIyTJuPRERk\n2LTbfNTc3OxLly6NugwRkWnl6aef3ufuLcdqN+1CYenSpaxduzbqMkREphUz21pKO20+EhGRYQoF\nEREZplAQEZFhCgURERmmUBARkWEKBRERGaZQEBGRYbEJhY27O7npgZdo6+6PuhQRkSkrNqHw832H\n+MJDm9l+sOfYjUVEYio2odBSmwJgX9d4rp0iIhIvsQmF5mxwUav9Xdp8JCJSTGxCoSkMBfUURESK\ni00o1KSSVFUm2H9IPQURkWJiEwpmRlNNmn2d6imIiBQTm1AAaK5N06rNRyIiRcUqFFqyKe1oFhEZ\nQ6xCoakmrR3NIiJjiFUoNNem2H+on1zOoy5FRGRKilcoZNMM5Zz2noGoSxERmZJiFQo6V0FEZGyx\nCoXmbH6oC+1sFhEZTcxCQT0FEZGxKBRERGRYrEKhIVNJMmE6V0FEpIhYhUIiYTTWpNRTEBEpIlah\nAMEmJO1oFhEZXQxDQT0FEZFiYhgKGupCRKSYGIaCBsUTESkmdqHQlE3TMzDEob7BqEsREZlyYhcK\nulaziEhxMQyFYKgLXWxHRORoMQwFndUsIlJMbENBm49ERI5WtlAws0Vm9pCZbTCz9Wb2p6O0MTP7\nvJltMrN1ZnZ2uerJa6zJj5SqnoKIyEgVZVz2IPCX7v6MmdUCT5vZg+6+oaDNJcDJ4e184Evhz7JJ\nVSSoz1QqFERERlG2noK773L3Z8L7ncCLwIIRzS4D7vLA40CDmc0rV015OldBRGR0k7JPwcyWAmcB\nT4yYtQDYVvB4O0cHB2Z2tZmtNbO1ra2tJ1xPUzato49EREZR9lAwsyzwXeDP3L3jeJbh7re6+yp3\nX9XS0nLCNbVk0+xXKIiIHKWsoWBmlQSB8A13/94oTXYAiwoeLwynlVUwKJ42H4mIjFTOo48MuA14\n0d3/oUizNcB7w6OQLgDa3X1XuWrKa8qmae8ZoH8wV+6XEhGZVsp59NEvAe8BnjezZ8NpHwUWA7j7\nLcB9wKXAJqAbeH8Z6xmWP1fhwKF+5tZXTcZLiohMC2ULBXf/KWDHaOPAh8tVQzFN2cPnKigUREQO\ni90ZzaChLkREiollKLQMh4J2NouIFIplKBRuPhIRkcNiGQo16QoylUmdqyAiMkIsQwGguVbnKoiI\njBTbUGiqSWvzkYjICLENheZsWj0FEZERYhwKKfUURERGiHEopDlwqJ9czqMuRURkyohxKKQYyjlt\nPQNRlyIiMmXENhSadFaziMhRYhsKs2uDUNjT0RtxJSIiU0dsQ2FhYzUA2w/2RFyJiMjUEdtQmFtX\nRWXSeO1Ad9SliIhMGbENhWTCWNCQYZtCQURkWGxDAWBRY7VCQUSkQOxDQZuPREQOi3cozKrmYPcA\nnb06V0FEBOIeCo0ZALYd0BFIIiIQ81BYOCt/WKo2IYmIQMxDYX59FQC7dQKbiAgQ81BozqapTBo7\n2xQKIiIQ81BIJIw5dVXsatc+BRERiHkoAMyvz7BLPQUREUChwLyGKnaqpyAiAigUmFefYU9Hry62\nIyKCQoH5DVUMDDn7Dum6CiIisQ+FuXXBYanaryAiolBgfkNwVrOOQBIRUSgwLzyBTecqiIgoFGis\nSZGuSKinICKCQgEzY159Fbva1VMQEYl9KEBwWKpCQUSkjKFgZreb2V4ze6HI/AvNrN3Mng1v15er\nlmOZ11DFrjZtPhIRqSjjsu8AbgbuGqPNT9z9rWWsoSTz6zPs6exjKOckExZ1OSIikSlbT8HdHwEO\nlGv5E2lufRVDOWdvpzYhiUi8Rb1PYbWZPWdm95vZ64o1MrOrzWytma1tbW2d8CLmN+iwVBERiDYU\nngGWuPsbgH8G/r1YQ3e/1d1XufuqlpaWCS9kQYOuwCYiAhGGgrt3uHtXeP8+oNLMmqOoZXFjEAqv\n7VcoiEi8RRYKZjbXzCy8f15Yy/4oasmkksytq2KLQkFEYq5sRx+Z2TeBC4FmM9sO3ABUArj7LcDl\nwAfNbBDoAa5098jGr17SVM3W/YeienkRkSmhbKHg7u8+xvybCQ5ZnRKWNtXwo5f2Rl2GiEikoj76\naMpY0lzNvq4+uvoGoy5FRCQyCoXQ0qYaAG1CEpFYUyiE8kcgbTugnc0iEl8KhdDs2jQArZ26LKeI\nxJdCIdRYk8IMWrv6oy5FRCQyCoVQRTLBrOoU+7rUUxCR+FIoFGjOptinzUciEmMKhQLN2bR6CiIS\nawqFAkEoaJ+CiMSXQqFAS616CiISbwqFAs3ZNN39Q3T366xmEYknhUKB5mwKgH2d2oQkIvGkUCjQ\nnD+BTZuQRCSmFAoFWrI6q1lE4q2kUDCzK0qZNt0tCsc/0qB4IhJXpfYUPlLitGmtPlNJczbN5tau\nqEsREYnEmBfZMbNLgEuBBWb2+YJZdcCMPERneUsNm1vVUxCReDpWT2EnsBboBZ4uuK0BLipvadFY\n1pLlVfUURCSmxuwpuPtzwHNm9q/uPgBgZrOARe5+cDIKnGzLW2o42D3AgUP9NNakoi5HRGRSlbpP\n4UEzqzOzRuAZ4Ctm9o9lrCsyy1uyAOotiEgslRoK9e7eAbwduMvdzwfeXL6yopMPBe1sFpE4KjUU\nKsxsHvBO4N4y1hO5BbMypCoSvKqdzSISQ6WGwieAB4DN7v6UmS0DXilfWdFJJoyTmmrUUxCRWBpz\nR3Oeu38H+E7B41eBd5SrqKgta6lh4+7OqMsQEZl0pZ7RvNDM/s3M9oa375rZwnIXF5XlLVm2Huim\nfzAXdSkiIpOq1M1HXyU4N2F+ePt+OG1GWtZSw1DOee2A9iuISLyUGgot7v5Vdx8Mb3cALWWsK1KH\nj0BSKIhIvJQaCvvN7PfNLBnefh/YX87CorQ4HBhv24HuiCsREZlcpYbCHxAcjrob2AVcDryvTDVF\nrqG6kkxlkl3tvVGXIiIyqUo6+ojgkNSr8kNbhGc2f4YgLGYcM2N+QxU723qiLkVEZFKV2lNYWTjW\nkbsfAM4qT0lTw/yGDDvVUxCRmCk1FBLhQHjAcE+h1F7GtDS/PqOegojETqkf7J8FHjOz/AlsVwCf\nLE9JU8P8hgytnX30DQ6RrkhGXY6IyKQoqafg7ncRDIa3J7y93d2/NtZzzOz28ES3F4rMNzP7vJlt\nMrN1Znb2eIsvp/kNVQDsadf1mkUkPkreBOTuG4AN41j2HcDNwF1F5l8CnBzezge+FP6cEuY3ZADY\n0dbD4qbqiKsREZkcpe5TGDd3fwQ4MEaTywiG4XZ3fxxoCEdinRIKQ0FEJC7KFgolWABsK3i8PZw2\nJSyclaEiYbrYjojESpShUDIzu9rM1prZ2tbW1kl5zcpkgiVN1Wzaq1AQkfiIMhR2AIsKHi8Mpx3F\n3W9191XuvqqlZfKGXFoxO8sm9RREJEaiDIU1wHvDo5AuANrdfVeE9RxlxewsW/drCG0RiY+ynYBm\nZt8ELgSazWw7cANQCeDutwD3AZcCm4Bu4P3lquV4rZidZSjnbN1/iJPn1EZdjohI2ZUtFNz93ceY\n78CHy/X6E2FFSxAEm/Z2KRREJBamxY7mqCyfXQOgnc0iEhsKhTFUpypY0JBhs3Y2i0hMKBSOYbmO\nQBKRGFEoHMOKliyb9x4il/OoSxERKTuFwjGsmJ2lZ2CIne0a7kJEZj6FwjGsmJ0FtLNZROJBoXAM\nCgURiROFwjE01qRorEnpCCQRiQWFQglWtGTVUxCRWFAolGD5bIWCiMSDQqEEK2ZnOdg9wP4uXZpT\nRGY2hUIJlrdouAsRiQeFQgmGj0DSzmYRmeEUCiWYX58hU5lUT0FEZjyFQgkSCWP57BqFgojMeAqF\nEgVjICkURGRmUyiUaMXsLDvbeznUNxh1KSIiZaNQKFF+Z7PObBaRmUyhUCKNgSQicaBQKNGSphoq\nk8bLexQKIjJzKRRKVJlMsLwly8t7OqMuRUSkbBQK43DKnFo27lYoiMjMpVAYh1Pn1rKjrYfO3oGo\nSxERKQuFwjicOqcWQJuQRGTGUiiMw6lzg1B4SZuQRGSGUiiMw8JZGWZVV/Lsa21RlyIiUhYKhXEw\nM85d2siTWw5EXYqISFkoFMbpvJMa2bq/mz0dvVGXIiIy4RQK43TeSY0APPlz9RZEZOZRKIzTGfPq\nqEklFQoiMiMpFMapIpngnKWNCgURmZEUCsfhvKWz2Link4OH+qMuRURkQikUjsN5JzUBsHbrwYgr\nERGZWAqF47ByYT0VCePZbQoFEZlZyhoKZnaxmW00s01mdt0o899nZq1m9mx4+0A565koVZVJ5tZX\nseNgT9SliIhMqIpyLdjMksAXgLcA24GnzGyNu28Y0fRud7+2XHWUy4KGDDvaFAoiMrOUs6dwHrDJ\n3V91937gW8BlZXy9SbWgIaOegojMOOUMhQXAtoLH28NpI73DzNaZ2T1mtmi0BZnZ1Wa21szWtra2\nlqPWcVswK8Pujl4Gh3JRlyIiMmGi3tH8fWCpu68EHgTuHK2Ru9/q7qvcfVVLS8ukFljM/IYMOYfd\nGu5CRGaQcobCDqDwm//CcNowd9/v7n3hw38BziljPRNqQUMGgJ1tCgURmTnKGQpPASeb2UlmlgKu\nBNYUNjCzeQUPfxt4sYz1TKj5YSjsaOuOuBIRkYlTtqOP3H3QzK4FHgCSwO3uvt7MPgGsdfc1wJ+Y\n2W8Dg8AB4H3lqmei5XsK2tksIjNJ2UIBwN3vA+4bMe36gvsfAT5SzhrKJZNKMq++ik17u6IuRURk\nwkS9o3laO31eHS/u0qU5RWTmUCicgDPm1bGptYvegaGoSxERmRAKhRNw+rw6hnKuTUgiMmMoFE7A\nGfPrANiwsyPiSkREJoZC4QQsaaymNl3Bk1t0wR0RmRkUCicgkTAuOXMu9z+/i+7+wajLERE5YQqF\nE3T5OYs41D/Efc/vjroUEZETplA4QecuncWpc2r5zAMbae8eiLocEZETolA4QWbGTVespLWrjy/+\neFPU5YiInBCFwgRYubCB1cuaePjlqTGst4jI8VIoTJDVy5t4aXcn+7v6jt1YRGSKUihMkAuWNQHw\nxM91eKqITF8KhQmycmE91akkj23eH3UpIiLHTaEwQSqTCc5d2shjryoURGT6UihMoNXLm9i0t4u9\nnboam4hMTwqFCbQ63K/w+KvaryAi05NCYQK9bn4dtekKfqJDU0VkmlIoTKCKZIKLXj+X+1/YrbGQ\nRGRaUihMsHeuWkRX3yD3aywkEZmGFAoT7Nyls1jWUsOXHt7MwFAu6nJERMZFoTDBzIyPXnI6m/Z2\n8bXHtkZdjojIuCgUyuDNp8/mnCWz+PbabQzlHHePuiQRkZIoFMrAzLj4dXN5aXcnF//TI/z1Peui\nLklEpCQKhTJ58+mzAXhlbxf3v7Bb+xdEZFpQKJTJspYsp82tpTmboqtvkKd0HWcRmQYUCmV09x+v\n5gd/9kZSyYQOURWRaaEi6gJmsvpMJQBvXTmPrz2+lfpMJX910akRVyUiUpxCYRL8/eUrqUwmuPmh\nTTRnU7zvl06KuiQRkVEpFCZBZTLB3739TA509/M3926gfyjH289eSHM2HXVpIiJH0D6FSZJMGJ+/\n8ixWLZnF3933Elfe+ji5nPP1x7fymQc2Rl2eiAigUJhUmVSSu69ezafefiab9nZx80ObuGHNem5+\naBOv7OmMujwREWy6nW27atUqX7t2bdRlnJDBoRxvuunH7GjrobEmxaG+QU6bV4cBmcokX//A+SQT\ndlzLPtQ3SE1aWwVF5Ehm9rS7rzpWO316RKAimeDm3z2L57a18WunzeE7T2/jrse20liT4tltbfzj\ngy+Tz4QPXriCvZ29bNjZwcWvn4tZ8bD41pOvccOa9dxzzS9y5sL6SVobEZlJytpTMLOLgc8BSeBf\n3P1TI+angbuAc4D9wLvcfctYy5wJPYVi3J23f+lRfvZaGwkDB+bWVdHZO0hX3yBXnLOQufVVvLir\nk1f3dfHOVYs4ZU6W/kGno2eAv/3PDXT2DvKbZ87j+t86g3994jV+9/zFpCsS1FVVkgiTpndgiDsf\n3cKZC+u54KSm4enjsau9h5p0BXVVlRP8W4jG2i0HmN+QYX5DJupSRMqi1J5C2ULBzJLAy8BbgO3A\nU8C73X1DQZsPASvd/RozuxJ4m7u/a6zlzuRQANjd3sv6ne2sWtrI89vbueuxLSTMyFZVcM/T20lY\nEBQtdVU8t63tiOc21aQ4f1kjP3hhN/WZSg52D1CZNAaGnFQyAQbLmmsAeGl3sA9j4awMy1uyvGFh\nPd39Q7T3DNA/lGNufRXuUJOqwHHaugeYW19FSzbNhl0dfO2xrbTUpnnv6iVkUkk6ewdJJozFjdVs\n3ttF98AQi2ZVU5epoLE6RSJhJBPG3o4+DhzqozKZoKtvkNbOPnZ39PKWM+aQqUzS1j1Ac22a9p4B\nsulkML+9j4Pd/WRSSRqrU7zp1BYqkwnWbW+jvWeA2bVpFjRUM6cuzaH+ITKVSWqrKhgYyvHo5v0k\nDOozKebUpZldV8Xejl4Gc86DG/awYnaWvR29/J//WE9NKskHfmUZbzq1hSWN1dSkKxjMOU+8up9M\nKsnpc+s42N1PfaaSphKPHOvoHWBXWy+ZyiQVSWNefdWYvb2poqtvkKEhp756eod+e88Az7x2kPn1\nGU6dW3vCy1u75QBPbTnIVb+4hOpU8Q0tg0M5ugeG8BxT5nc4FUJhNXCju18UPv4IgLv/v4I2D4Rt\nHjOzCmA30OJjFDXTQ2EsPf1DpCsSJBKGu7OrvZc9Hb2kKhJk0xUsaMhwqH+Iz/7XRrYf7OHycxby\n6OZ9LGiopq2nn1zOeXlPF9sPdvOhC1eQTBj3rtvJjrZeXtrdQSqZoLEmRWUywc62HhIJo38wGLMp\nm66gqy+4mlwyYVz0ujn87LU2drX3HlWnGVQkgjA6lopEEHht3QNjtqtNV9A3mKO/TGNIrV7WRE06\nyQ9f3FtS+5pUcvjD/YiPeDviB119g+QKfg2pigQVCSNhhhH8rhLh44QFgykmDAzDjLBNsLScO/u6\n+mioTgUhP8JoWTPqNI6eOLLdjoM9DOacbLpiuE4bUSPkHzO8Psmk4Q5t3QP0D+ZoqU0PL3u0/+rC\nf/Vify350vK/B3fHw+U5Hv48vHwzSIY17uroHZ4+py5NRSJBR+8AA0M5sulK0hWHf4/5Ogt/F/nf\nVX7atgPd5Dx4HysTRk26gu7+ITKpJKlkgt6BIQ71D9I7cPjvdH59FVWVyaPWMb/uo6134TpbwYR3\nn7uYP3rjsiK/qbFNhX0KC4BtBY+3A+cXa+Pug2bWDjQB+wobmdnVwNUAixcvLle9U14mlRy+b2aj\nbu6ozyT4xGWvH3586Znzxlzm75y1AIDO3gGqUxXDO7iHck7CoH8oR9KMimSCg4f6aesZYE5dmupU\nBf2DOXoHh+gdGKKqMkku5+xs66W5NhX0VA4NcLC7n/aeAXI5Z8iduqpK5tZX0T+Yo7aqAjOjqjLB\ny7u7GMjlqKuqYH9XP7NqgjGjWrJp5tRVkQr/eXe19/DTV/aRc2flwgaas2n2dvay/WAPezt6qUlX\n0DuQo7N3gCF3zlvaSCYV9EB2t/eyp7OXlvBb/jlLZrG59RDpigSrlzdRmUywo62HF3d28NqBbnoH\nhzCM0+bWDvdqGqorae3sY29nH3DkB52H/96F0+oylayYnaVvYIjewRzbD3STcyfnwYe8e/DhkH+c\nG3585Ied4yTMaKpJ0dY9wGDuyI8SH+2jpbRJRw3t7gR/N3VVwboOf/CGH8b52oISnFwueP2cB383\nAA3VlVQmE+zr6jti2YWBNPwhXDh/RDjlS/OCx4fDsiA8w09Os6DOoZwzlINFjRnOXdrIuu3tbNl3\nKAy6JKmKoKfaP5j/YB7xQke85uGJl545jwuWNfHwxlYc51DfINl0JT0Dg/QN5qhOJalJVVCdqqAm\nnWQw57y0q4PC70ejrW/htMJ1dY4Mj5ba8p/bVM6ewuXAxe7+gfDxe4Dz3f3agjYvhG22h483h232\njbZMiHdPQUTkeJXaUyjneQo7gEUFjxeG00ZtE24+qifY4SwiIhEoZyg8BZxsZieZWQq4Elgzos0a\n4Krw/uXAf4+1P0FERMqrbPsUwn0E1wIPEBySeru7rzezTwBr3X0NcBvwNTPbBBwgCA4REYlIWU9e\nc/f7gPtGTLu+4H4vcEU5axARkdJp7CMRERmmUBARkWEKBRERGaZQEBGRYdNu6GwzawW2HufTmxlx\ntvQ0pnWZmrQuU5PWBZa4e8uxGk27UDgRZra2lDP6pgOty9SkdZmatC6l0+YjEREZplAQEZFhcQuF\nW6MuYAJpXaYmrcvUpHUpUaz2KYiIyNji1lMQEZExKBRERGRYbELBzC42s41mtsnMrou6nvEysy1m\n9ryZPWtma8NpjWb2oJm9Ev6cFXWdozGz281sb3hRpfy0UWu3wOfD92mdmZ0dXeVHK7IuN5rZjvC9\nedbMLi2Y95FwXTaa2UXRVH00M1tkZg+Z2QYzW29mfxpOn3bvyxjrMh3flyoze9LMngvX5W/C6SeZ\n2RNhzXeHlyPAzNLh403h/KUnXIS7z/gbwdDdm4FlQAp4Djgj6rrGuQ5bgOYR0z4NXBfevw74+6jr\nLFL7G4GzgReOVTtwKXA/wRUKLwCeiLr+EtblRuCvRml7Rvi3lgZOCv8Gk1GvQ1jbPODs8H4t8HJY\n77R7X8ZYl+n4vhiQDe9XAk+Ev+9vA1eG028BPhje/xBwS3j/SuDuE60hLj2F84BN7v6qu/cD3wIu\ni7imiXAZcGd4/07gdyKspSh3f4TgehmFitV+GXCXBx4HGsxs7AtNT6Ii61LMZcC33L3P3X8ObCL4\nW4ycu+9y92fC+53AiwTXTJ9278sY61LMVH5f3N27woeV4c2BXwPuCaePfF/y79c9wJvNRl7penzi\nEgoLgG0Fj7cz9h/NVOTAf5nZ02Z2dThtjrvvCu/vBuZEU9pxKVb7dH2vrg03q9xesBlvWqxLuMnh\nLIJvpdP6fRmxLjAN3xczS5pYbJxfAAAGEElEQVTZs8Be4EGCnkybuw+GTQrrHV6XcH470HQirx+X\nUJgJftndzwYuAT5sZm8snOlB/3FaHl88nWsPfQlYDvwCsAv4bLTllM7MssB3gT9z947CedPtfRll\nXabl++LuQ+7+CwTXtT8POG0yXz8uobADWFTweGE4bdpw9x3hz73AvxH8sezJd+HDn3ujq3DcitU+\n7d4rd98T/iPngK9weFPElF4XM6sk+BD9hrt/L5w8Ld+X0dZlur4vee7eBjwErCbYXJe/UmZhvcPr\nEs6vB/afyOvGJRSeAk4O9+CnCHbIrIm4ppKZWY2Z1ebvA78BvECwDleFza4C/iOaCo9LsdrXAO8N\nj3a5AGgv2JwxJY3Ytv42gvcGgnW5MjxC5CTgZODJya5vNOF259uAF939HwpmTbv3pdi6TNP3pcXM\nGsL7GeAtBPtIHgIuD5uNfF/y79flwH+HPbzjF/Xe9sm6ERw98TLB9rmPRV3POGtfRnC0xHPA+nz9\nBNsOfwS8AvwQaIy61iL1f5Og+z5AsD30D4vVTnD0xRfC9+l5YFXU9ZewLl8La10X/pPOK2j/sXBd\nNgKXRF1/QV2/TLBpaB3wbHi7dDq+L2Osy3R8X1YCPwtrfgG4Ppy+jCC4NgHfAdLh9Krw8aZw/rIT\nrUHDXIiIyLC4bD4SEZESKBRERGSYQkFERIYpFEREZJhCQUREhikUZMows0fDn0vN7HcneNkfHe21\nysXMfsfMri/Tsj967FbjXuaZZnbHRC9Xph8dkipTjpldSDC65VvH8ZwKPzw2zGjzu9w9OxH1lVjP\no8Bvu/u+E1zOUetVrnUxsx8Cf+Dur030smX6UE9Bpgwzy48O+SngV8Ix8P88HCDsJjN7Khzc7I/D\n9hea2U/MbA2wIZz27+GggevzAwea2aeATLi8bxS+VniG7k1m9oIF16t4V8Gyf2xm95jZS2b2jfzo\nk2b2KQvG7l9nZp8ZZT1OAfrygWBmd5jZLWa21sxeNrO3htNLXq+CZY+2Lr9vwRj8z5rZl80smV9H\nM/ukBWPzP25mc8LpV4Tr+5yZPVKw+O8TnO0vcRb1GXy66Za/AV3hzwuBewumXw18PLyfBtYSjIN/\nIXAIOKmgbf4M3AzBGaFNhcse5bXeQTASZZJgRNDXCMbnv5BgxMmFBF+eHiM4c7aJ4CzYfC+7YZT1\neD/w2YLHdwA/CJdzMsGZ0FXjWa/Rag/vn07wYV4ZPv4i8N7wvgO/Fd7/dMFrPQ8sGFk/8EvA96P+\nO9At2lt+gCWRqew3gJVmlh/7pZ7gw7UfeNKDMfHz/sTM3hbeXxS2G2uAsF8GvunuQwSDwT0MnAt0\nhMveDmDBUMZLgceBXuA2M7sXuHeUZc4DWkdM+7YHA7O9YmavEox8OZ71KubNwDnAU2FHJsPhQez6\nC+p7mmAcHYD/Ae4ws28D3zu8KPYC80t4TZnBFAoyHRjwv9z9gSMmBvseDo14/OvAanfvNrMfE3wj\nP159BfeHgAp3HzSz8wg+jC8HriW4AEqhHoIP+EIjd945Ja7XMRhwp7t/ZJR5A+6ef90hwv93d7/G\nzM4HfhN42szOcff9BL+rnhJfV2Yo7VOQqaiT4LKKeQ8AH7RgeGTM7JRwtNiR6oGDYSCcRnAZw7yB\n/PNH+AnwrnD7fgvB5TaLjphpwZj99e5+H/DnwBtGafYisGLEtCvMLGFmywkGN9s4jvUaqXBdfgRc\nbmazw2U0mtmSsZ5sZsvd/Ql3v56gR5MfRvoUDo8kKjGlnoJMReuAITN7jmB7/OcINt08E+7sbWX0\nS4/+ALjGzF4k+NB9vGDercA6M3vG3X+vYPq/EYxX/xzBt/e/dvfdYaiMphb4DzOrIviW/hejtHkE\n+KyZWcE39dcIwqYOuMbde83sX0pcr5GOWBcz+zjBVfkSBKO3fhjYOsbzbzKzk8P6fxSuO8CvAv9Z\nwuvLDKZDUkXKwMw+R7DT9ofh8f/3uvs9x3haZMwsDTxMcIW/oof2ysynzUci5fF3QHXURYzDYuA6\nBYKopyAiIsPUUxARkWEKBRERGaZQEBGRYQoFEREZplAQEZFh/x+IilA0zKuBpwAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.9990884\n",
            "Test Accuracy: 0.44852942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "5056a1ecc0f5eb241aa265e2bb4323af",
          "grade": true,
          "grade_id": "1f",
          "locked": false,
          "points": 10,
          "schema_version": 1,
          "solution": true
        },
        "id": "nsaoIcaJMcKG",
        "colab_type": "text"
      },
      "source": [
        "In the first part we've implemented a fully connected NN architecture of shape. <br>\n",
        "67500 (image dimension 150\\*150\\*3) | 2048 (layer-1) | 1024 (layer-2) | 512 (layer-3) | 512 (layer-4) | 256 (layer-5) | 64 (layer-6) | 10 (softmax-layer).<br>\n",
        "\n",
        "#### We can see that It over-fitted very badly. We got 45-50% accuracy using the above method. The model is overfitting, because the dev accuracy is 99.9% and validation accuracy is 45-50% <br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "10e364dd0cb41f5ea572c494acf33e20",
          "grade": false,
          "grade_id": "cell-326d261ff85c6db7",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "UhjrKM-3McKH",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c509df027738d93e33e449af8b1dea3d",
          "grade": false,
          "grade_id": "cell-979f0bd35b7c857d",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "o_l0aJyyMcKI",
        "colab_type": "text"
      },
      "source": [
        "# Part 2 - CNN Architecture\n",
        "In part 2 we'll be training an advanced architecture such as CNN to perform the same task from part 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "510c87ee6bb8952f2e123297e71f33e5",
          "grade": false,
          "grade_id": "cell-6386fe54ffcfe0f0",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "F4N_bgyxMcKI",
        "colab_type": "text"
      },
      "source": [
        "# 2A - Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "c6401fdc9ad5f687e1d87b27b44b0de6",
          "grade": true,
          "grade_id": "2a",
          "locked": false,
          "points": 8,
          "schema_version": 1,
          "solution": true
        },
        "id": "gr9MDOTQMcKK",
        "colab_type": "code",
        "outputId": "6c168810-6b9f-4621-d725-682f97ae412e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import math                      # providing access to the mathematical functions defined by the C standard\n",
        "import matplotlib.pyplot as plt  # plotting library\n",
        "import scipy                     # scientific computnig and technical computing\n",
        "import cv2                       # working with, mainly resizing, images\n",
        "import numpy as np               # dealing with arrays\n",
        "import glob                      # return a possibly-empty list of path names that match pathname\n",
        "import os                        # dealing with directories\n",
        "import pandas as pd              # providing data structures and data analysis tools\n",
        "import tensorflow as tf       \n",
        "import itertools\n",
        "import random\n",
        "from random import shuffle       # mixing up or currently ordered data that might lead our network astray in training.\n",
        "from tqdm import tqdm            # a nice pretty percentage bar for tasks. Thanks to viewer Daniel Bühler for this suggestion\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import metrics\n",
        "%matplotlib inline\n",
        "np.random.seed(1)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'Colab Notebooks/CNNMonkeySpeciesClassification/'\n",
        "dirpath = base_dir\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Defining directory paths-----------------------------------------------------------------------------------------------------------------\n",
        "labels_path = dirpath+r'/monkey_labels.txt'\n",
        "train_dir = dirpath+'/training/'\n",
        "test_dir = dirpath+'/validation/'\n",
        "\n",
        "# Reading monkey_labels.txt using Pandas---------------------------------------------------------------------------------------------------\n",
        "cols = ['label','latinName', 'commonName','training_images', 'validation_images']\n",
        "labels = pd.read_csv(labels_path, names=cols, sep=',', skiprows=1)\n",
        "\n",
        "\n",
        "# Training generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    target_size=(height,width),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    seed=seed,\n",
        "                                                    shuffle=True,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "# Test generator\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                  target_size=(height,width), \n",
        "                                                  batch_size=batch_size,\n",
        "                                                  seed=seed,\n",
        "                                                  shuffle=False,\n",
        "                                                  class_mode='categorical')\n",
        "\n",
        "train_num = train_generator.samples\n",
        "validation_num = validation_generator.samples "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Found 1097 images belonging to 10 classes.\n",
            "Found 272 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "bec3b5493e239c1028ebe30ba7e3918b",
          "grade": false,
          "grade_id": "cell-f934f1e502453cae",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "vfvsB-YPMcKN",
        "colab_type": "text"
      },
      "source": [
        "# 2B - Model definition\n",
        "The CNN model consists of 4 Conv layers followed by 2 dense fully-connected layers. I've printed the configuration below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "8661e068cfe98df6c4c97f6f297321e2",
          "grade": true,
          "grade_id": "2b",
          "locked": false,
          "points": 8,
          "schema_version": 1,
          "solution": true
        },
        "id": "m9leLsKbMcKO",
        "colab_type": "code",
        "outputId": "92cabdc5-8f45-4061-8683-0d7166511053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "source": [
        "\n",
        "# Model Definition----------------------------------------------------------------------------------------------------------------------------------------------\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 148, 148, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 72, 72, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 72, 72, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 36, 36, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 36, 36, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 34, 34, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 34, 34, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 18496)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               9470464   \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,541,162\n",
            "Trainable params: 9,541,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b9d1083587628091653eb3563f18b7be",
          "grade": false,
          "grade_id": "cell-7d0e60aef3748e6e",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "YagONDSyMcKS",
        "colab_type": "text"
      },
      "source": [
        "# 2C - Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "21601801bac726b08e5c694d6e1688f6",
          "grade": true,
          "grade_id": "2c",
          "locked": false,
          "points": 8,
          "schema_version": 1,
          "solution": true
        },
        "id": "wb20_RBvMcKS",
        "colab_type": "code",
        "outputId": "44d14a39-fbb6-480f-d3c6-d21d764c5bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13527
        }
      },
      "source": [
        "LR = 1e-3\n",
        "height=150\n",
        "width=150\n",
        "channels=3\n",
        "seed=1337\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "\n",
        "filepath=str(os.getcwd()+\"/model.h5f\")\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "# = EarlyStopping(monitor='val_acc', patience=15)\n",
        "callbacks_list = [checkpoint]#, stopper]\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch= train_num // batch_size,\n",
        "                              epochs=epochs,\n",
        "                              validation_data=train_generator,\n",
        "                              validation_steps= validation_num // batch_size,\n",
        "                              callbacks=callbacks_list, \n",
        "                              verbose = 1\n",
        "                             )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/200\n",
            "17/17 [==============================] - 200s 12s/step - loss: 2.3149 - acc: 0.1167 - val_loss: 2.2480 - val_acc: 0.1992\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.19922, saving model to /content/model.h5f\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 2.1418 - acc: 0.2070 - val_loss: 2.0271 - val_acc: 0.2656\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.19922 to 0.26562, saving model to /content/model.h5f\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 35s 2s/step - loss: 1.9541 - acc: 0.2914 - val_loss: 1.7930 - val_acc: 0.4030\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.26562 to 0.40299, saving model to /content/model.h5f\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 1.8209 - acc: 0.3494 - val_loss: 1.8296 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.40299\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 1.7991 - acc: 0.3346 - val_loss: 2.0384 - val_acc: 0.2260\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.40299\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 1.7305 - acc: 0.3738 - val_loss: 1.5441 - val_acc: 0.4570\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.40299 to 0.45703, saving model to /content/model.h5f\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 1.7251 - acc: 0.3761 - val_loss: 1.6535 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.45703\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 1.5758 - acc: 0.4256 - val_loss: 1.4034 - val_acc: 0.5352\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.45703 to 0.53516, saving model to /content/model.h5f\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 1.5058 - acc: 0.4540 - val_loss: 1.3873 - val_acc: 0.4922\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.53516\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 1.4080 - acc: 0.4842 - val_loss: 1.2960 - val_acc: 0.5174\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.53516\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 1.5235 - acc: 0.4494 - val_loss: 1.3328 - val_acc: 0.5959\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.53516 to 0.59589, saving model to /content/model.h5f\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 1.4192 - acc: 0.5047 - val_loss: 1.2620 - val_acc: 0.5586\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.59589\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 1.4334 - acc: 0.4685 - val_loss: 1.2645 - val_acc: 0.5195\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.59589\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 1.3268 - acc: 0.5006 - val_loss: 1.1865 - val_acc: 0.5781\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.59589\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 1.2950 - acc: 0.5248 - val_loss: 1.0430 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.59589 to 0.62500, saving model to /content/model.h5f\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 1.2009 - acc: 0.5566 - val_loss: 1.2692 - val_acc: 0.5273\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.62500\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 1.2455 - acc: 0.5520 - val_loss: 1.2416 - val_acc: 0.5342\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.62500\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 1.2531 - acc: 0.5727 - val_loss: 1.1581 - val_acc: 0.5820\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.62500\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 1.2119 - acc: 0.5679 - val_loss: 1.1201 - val_acc: 0.5898\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.62500\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 1.2170 - acc: 0.5683 - val_loss: 1.1583 - val_acc: 0.6016\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.62500\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 1.2449 - acc: 0.5598 - val_loss: 1.1107 - val_acc: 0.6094\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.62500\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 1.1292 - acc: 0.5875 - val_loss: 1.1111 - val_acc: 0.5586\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.62500\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 1.0833 - acc: 0.6080 - val_loss: 0.9003 - val_acc: 0.6866\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.62500 to 0.68657, saving model to /content/model.h5f\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 1.0797 - acc: 0.6209 - val_loss: 1.1112 - val_acc: 0.6133\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.68657\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 1.2484 - acc: 0.5623 - val_loss: 1.1633 - val_acc: 0.5820\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.68657\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 1.1120 - acc: 0.5877 - val_loss: 0.9292 - val_acc: 0.6719\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.68657\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 1.0741 - acc: 0.6195 - val_loss: 1.0900 - val_acc: 0.6211\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.68657\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 1.0437 - acc: 0.6103 - val_loss: 0.8601 - val_acc: 0.7227\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.68657 to 0.72266, saving model to /content/model.h5f\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 1.0577 - acc: 0.6225 - val_loss: 0.9063 - val_acc: 0.6781\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.72266\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 1.1425 - acc: 0.5828 - val_loss: 1.0740 - val_acc: 0.6406\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.72266\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 1.0742 - acc: 0.6264 - val_loss: 0.8710 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.72266\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 1.0580 - acc: 0.6149 - val_loss: 1.0316 - val_acc: 0.6406\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.72266\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 1.0100 - acc: 0.6443 - val_loss: 0.8358 - val_acc: 0.7109\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.72266\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 29s 2s/step - loss: 0.8927 - acc: 0.6769 - val_loss: 0.8875 - val_acc: 0.6758\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.72266\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.9685 - acc: 0.6585 - val_loss: 1.0655 - val_acc: 0.6096\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.72266\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.9354 - acc: 0.6612 - val_loss: 0.8598 - val_acc: 0.6914\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.72266\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 1.0400 - acc: 0.6435 - val_loss: 0.8182 - val_acc: 0.7383\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.72266 to 0.73828, saving model to /content/model.h5f\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.9363 - acc: 0.6714 - val_loss: 0.8409 - val_acc: 0.6914\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.73828\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.8862 - acc: 0.6707 - val_loss: 0.7984 - val_acc: 0.6914\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.73828\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.9781 - acc: 0.6455 - val_loss: 1.1607 - val_acc: 0.5859\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.73828\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 1.0117 - acc: 0.6354 - val_loss: 0.9035 - val_acc: 0.6418\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.73828\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.9636 - acc: 0.6497 - val_loss: 0.8779 - val_acc: 0.7148\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.73828\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.8697 - acc: 0.6732 - val_loss: 0.8616 - val_acc: 0.6602\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.73828\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.8333 - acc: 0.7059 - val_loss: 0.8360 - val_acc: 0.7188\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.73828\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.8319 - acc: 0.6976 - val_loss: 0.7210 - val_acc: 0.7305\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.73828\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.7845 - acc: 0.7092 - val_loss: 0.7958 - val_acc: 0.7114\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.73828\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.8365 - acc: 0.6926 - val_loss: 0.7450 - val_acc: 0.7603\n",
            "\n",
            "Epoch 00047: val_acc improved from 0.73828 to 0.76027, saving model to /content/model.h5f\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 0.8252 - acc: 0.7207 - val_loss: 0.7525 - val_acc: 0.7031\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.76027\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.7824 - acc: 0.7244 - val_loss: 0.7620 - val_acc: 0.7305\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.76027\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.8135 - acc: 0.7089 - val_loss: 0.7246 - val_acc: 0.7461\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.76027\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.7707 - acc: 0.7252 - val_loss: 0.5052 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00051: val_acc improved from 0.76027 to 0.84766, saving model to /content/model.h5f\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.8365 - acc: 0.6896 - val_loss: 0.8152 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.84766\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.9205 - acc: 0.6667 - val_loss: 0.6739 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.84766\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.8459 - acc: 0.6866 - val_loss: 0.6752 - val_acc: 0.7734\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.84766\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.7837 - acc: 0.7101 - val_loss: 0.6916 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.84766\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.7590 - acc: 0.7248 - val_loss: 0.5534 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.84766\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.6928 - acc: 0.7622 - val_loss: 0.6200 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.84766\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 29s 2s/step - loss: 0.7297 - acc: 0.7333 - val_loss: 0.6630 - val_acc: 0.7773\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.84766\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 0.6982 - acc: 0.7393 - val_loss: 0.5482 - val_acc: 0.7861\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.84766\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.7139 - acc: 0.7645 - val_loss: 0.6509 - val_acc: 0.7773\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.84766\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.7049 - acc: 0.7506 - val_loss: 0.5701 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.84766\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.6788 - acc: 0.7529 - val_loss: 0.7220 - val_acc: 0.7188\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.84766\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.7015 - acc: 0.7518 - val_loss: 0.5108 - val_acc: 0.8164\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.84766\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.6279 - acc: 0.7675 - val_loss: 0.6418 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.84766\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.6753 - acc: 0.7645 - val_loss: 0.5278 - val_acc: 0.8219\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.84766\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.7690 - acc: 0.7200 - val_loss: 0.6553 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.84766\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.7092 - acc: 0.7467 - val_loss: 0.6365 - val_acc: 0.7539\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.84766\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.6932 - acc: 0.7541 - val_loss: 0.5712 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.84766\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 0.6367 - acc: 0.7702 - val_loss: 0.4920 - val_acc: 0.8398\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.84766\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 29s 2s/step - loss: 0.6223 - acc: 0.7783 - val_loss: 0.6348 - val_acc: 0.7695\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.84766\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.6973 - acc: 0.7562 - val_loss: 0.4751 - val_acc: 0.8356\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.84766\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.6337 - acc: 0.7735 - val_loss: 0.3958 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00072: val_acc improved from 0.84766 to 0.87109, saving model to /content/model.h5f\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.5959 - acc: 0.7965 - val_loss: 0.5253 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.87109\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.7081 - acc: 0.7324 - val_loss: 0.4617 - val_acc: 0.8203\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.87109\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.6471 - acc: 0.7721 - val_loss: 0.5020 - val_acc: 0.8203\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.87109\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.7058 - acc: 0.7485 - val_loss: 0.5985 - val_acc: 0.7734\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.87109\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.8297 - acc: 0.7239 - val_loss: 0.6713 - val_acc: 0.7861\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.87109\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.7547 - acc: 0.7419 - val_loss: 0.5814 - val_acc: 0.7578\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.87109\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.6148 - acc: 0.7921 - val_loss: 0.4548 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.87109\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.6233 - acc: 0.7698 - val_loss: 0.4707 - val_acc: 0.8242\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.87109\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.5801 - acc: 0.7827 - val_loss: 0.4280 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.87109\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.6096 - acc: 0.7815 - val_loss: 0.5282 - val_acc: 0.8242\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.87109\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.6580 - acc: 0.7681 - val_loss: 0.4997 - val_acc: 0.8209\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.87109\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.5695 - acc: 0.7914 - val_loss: 0.4744 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.87109\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.5144 - acc: 0.8094 - val_loss: 0.3664 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00085: val_acc improved from 0.87109 to 0.88281, saving model to /content/model.h5f\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.5470 - acc: 0.8061 - val_loss: 0.3709 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.88281\n",
            "Epoch 87/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.5958 - acc: 0.7875 - val_loss: 0.4338 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.88281\n",
            "Epoch 88/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.5602 - acc: 0.8069 - val_loss: 0.4787 - val_acc: 0.8507\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.88281\n",
            "Epoch 89/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.5667 - acc: 0.8094 - val_loss: 0.3836 - val_acc: 0.8756\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.88281\n",
            "Epoch 90/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.5367 - acc: 0.8023 - val_loss: 0.3734 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.88281\n",
            "Epoch 91/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.5763 - acc: 0.7919 - val_loss: 0.4466 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.88281\n",
            "Epoch 92/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.5183 - acc: 0.8079 - val_loss: 0.3563 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.88281\n",
            "Epoch 93/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.5171 - acc: 0.8106 - val_loss: 0.3458 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.88281\n",
            "Epoch 94/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4845 - acc: 0.8232 - val_loss: 0.3322 - val_acc: 0.8955\n",
            "\n",
            "Epoch 00094: val_acc improved from 0.88281 to 0.89552, saving model to /content/model.h5f\n",
            "Epoch 95/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.5123 - acc: 0.8168 - val_loss: 0.4219 - val_acc: 0.8806\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.89552\n",
            "Epoch 96/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.5275 - acc: 0.8214 - val_loss: 0.3728 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.89552\n",
            "Epoch 97/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.5945 - acc: 0.7896 - val_loss: 0.4195 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.89552\n",
            "Epoch 98/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.5573 - acc: 0.7882 - val_loss: 0.3929 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.89552\n",
            "Epoch 99/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.5117 - acc: 0.8033 - val_loss: 0.3330 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00099: val_acc improved from 0.89552 to 0.89844, saving model to /content/model.h5f\n",
            "Epoch 100/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4696 - acc: 0.8322 - val_loss: 0.3879 - val_acc: 0.8633\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.89844\n",
            "Epoch 101/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.5151 - acc: 0.8244 - val_loss: 0.4034 - val_acc: 0.8630\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.89844\n",
            "Epoch 102/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 0.5712 - acc: 0.8087 - val_loss: 0.4847 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.89844\n",
            "Epoch 103/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.4907 - acc: 0.8359 - val_loss: 0.3598 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.89844\n",
            "Epoch 104/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.5048 - acc: 0.8200 - val_loss: 0.3475 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.89844\n",
            "Epoch 105/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4793 - acc: 0.8297 - val_loss: 0.3081 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.89844\n",
            "Epoch 106/200\n",
            "17/17 [==============================] - 29s 2s/step - loss: 0.4698 - acc: 0.8301 - val_loss: 0.8953 - val_acc: 0.7773\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.89844\n",
            "Epoch 107/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.9313 - acc: 0.7221 - val_loss: 0.5930 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.89844\n",
            "Epoch 108/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.5953 - acc: 0.7836 - val_loss: 0.4169 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.89844\n",
            "Epoch 109/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.4999 - acc: 0.8087 - val_loss: 0.3277 - val_acc: 0.8789\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.89844\n",
            "Epoch 110/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.5002 - acc: 0.8355 - val_loss: 0.3766 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.89844\n",
            "Epoch 111/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 0.3888 - acc: 0.8617 - val_loss: 0.2541 - val_acc: 0.8945\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.89844\n",
            "Epoch 112/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4373 - acc: 0.8461 - val_loss: 0.2540 - val_acc: 0.9303\n",
            "\n",
            "Epoch 00112: val_acc improved from 0.89844 to 0.93035, saving model to /content/model.h5f\n",
            "Epoch 113/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4386 - acc: 0.8447 - val_loss: 0.3638 - val_acc: 0.8557\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.93035\n",
            "Epoch 114/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.4461 - acc: 0.8308 - val_loss: 0.2180 - val_acc: 0.9531\n",
            "\n",
            "Epoch 00114: val_acc improved from 0.93035 to 0.95312, saving model to /content/model.h5f\n",
            "Epoch 115/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4288 - acc: 0.8548 - val_loss: 0.3136 - val_acc: 0.8867\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.95312\n",
            "Epoch 116/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.3937 - acc: 0.8640 - val_loss: 0.2275 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.95312\n",
            "Epoch 117/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3875 - acc: 0.8622 - val_loss: 0.2498 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.95312\n",
            "Epoch 118/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.3838 - acc: 0.8613 - val_loss: 0.3548 - val_acc: 0.8706\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.95312\n",
            "Epoch 119/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 0.3473 - acc: 0.8719 - val_loss: 0.2197 - val_acc: 0.9254\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.95312\n",
            "Epoch 120/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3532 - acc: 0.8739 - val_loss: 0.2878 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.95312\n",
            "Epoch 121/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3945 - acc: 0.8583 - val_loss: 0.2478 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.95312\n",
            "Epoch 122/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.3573 - acc: 0.8722 - val_loss: 0.2939 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.95312\n",
            "Epoch 123/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4127 - acc: 0.8583 - val_loss: 0.2503 - val_acc: 0.9023\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.95312\n",
            "Epoch 124/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4215 - acc: 0.8364 - val_loss: 0.2656 - val_acc: 0.9055\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.95312\n",
            "Epoch 125/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.4700 - acc: 0.8336 - val_loss: 0.2931 - val_acc: 0.9104\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.95312\n",
            "Epoch 126/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4265 - acc: 0.8500 - val_loss: 0.2552 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.95312\n",
            "Epoch 127/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.4215 - acc: 0.8520 - val_loss: 0.2623 - val_acc: 0.9102\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.95312\n",
            "Epoch 128/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.3848 - acc: 0.8675 - val_loss: 0.2333 - val_acc: 0.9180\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.95312\n",
            "Epoch 129/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 0.4101 - acc: 0.8557 - val_loss: 0.3096 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.95312\n",
            "Epoch 130/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.4192 - acc: 0.8569 - val_loss: 0.2748 - val_acc: 0.9180\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.95312\n",
            "Epoch 131/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.4293 - acc: 0.8500 - val_loss: 0.2910 - val_acc: 0.8767\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.95312\n",
            "Epoch 132/200\n",
            "17/17 [==============================] - 34s 2s/step - loss: 0.3940 - acc: 0.8673 - val_loss: 0.3183 - val_acc: 0.8867\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.95312\n",
            "Epoch 133/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.3625 - acc: 0.8792 - val_loss: 0.2483 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.95312\n",
            "Epoch 134/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.3701 - acc: 0.8622 - val_loss: 0.2435 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.95312\n",
            "Epoch 135/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.3845 - acc: 0.8702 - val_loss: 0.2603 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.95312\n",
            "Epoch 136/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.4126 - acc: 0.8730 - val_loss: 0.2278 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.95312\n",
            "Epoch 137/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3863 - acc: 0.8700 - val_loss: 0.2920 - val_acc: 0.9005\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.95312\n",
            "Epoch 138/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3529 - acc: 0.8749 - val_loss: 0.2871 - val_acc: 0.9023\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.95312\n",
            "Epoch 139/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.3984 - acc: 0.8629 - val_loss: 0.3265 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.95312\n",
            "Epoch 140/200\n",
            "17/17 [==============================] - 35s 2s/step - loss: 0.3600 - acc: 0.8621 - val_loss: 0.1908 - val_acc: 0.9414\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.95312\n",
            "Epoch 141/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3768 - acc: 0.8686 - val_loss: 0.2140 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.95312\n",
            "Epoch 142/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3173 - acc: 0.8804 - val_loss: 0.2640 - val_acc: 0.9303\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.95312\n",
            "Epoch 143/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.3426 - acc: 0.8686 - val_loss: 0.1581 - val_acc: 0.9452\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.95312\n",
            "Epoch 144/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3257 - acc: 0.8885 - val_loss: 0.2427 - val_acc: 0.9023\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.95312\n",
            "Epoch 145/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.3364 - acc: 0.8776 - val_loss: 0.2267 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.95312\n",
            "Epoch 146/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.4260 - acc: 0.8573 - val_loss: 0.2271 - val_acc: 0.9336\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.95312\n",
            "Epoch 147/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2762 - acc: 0.9097 - val_loss: 0.1658 - val_acc: 0.9648\n",
            "\n",
            "Epoch 00147: val_acc improved from 0.95312 to 0.96484, saving model to /content/model.h5f\n",
            "Epoch 148/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.3274 - acc: 0.8887 - val_loss: 0.2019 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.96484\n",
            "Epoch 149/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.3019 - acc: 0.8958 - val_loss: 0.1514 - val_acc: 0.9552\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.96484\n",
            "Epoch 150/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3990 - acc: 0.8585 - val_loss: 0.2592 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.96484\n",
            "Epoch 151/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3847 - acc: 0.8739 - val_loss: 0.2463 - val_acc: 0.9023\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.96484\n",
            "Epoch 152/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3641 - acc: 0.8742 - val_loss: 0.1653 - val_acc: 0.9492\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.96484\n",
            "Epoch 153/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.3077 - acc: 0.8977 - val_loss: 0.1826 - val_acc: 0.9336\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.96484\n",
            "Epoch 154/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.3156 - acc: 0.9004 - val_loss: 0.1478 - val_acc: 0.9648\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.96484\n",
            "Epoch 155/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.2956 - acc: 0.9009 - val_loss: 0.2680 - val_acc: 0.9055\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.96484\n",
            "Epoch 156/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.3958 - acc: 0.8640 - val_loss: 0.2915 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.96484\n",
            "Epoch 157/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3686 - acc: 0.8702 - val_loss: 0.2084 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.96484\n",
            "Epoch 158/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.3277 - acc: 0.8869 - val_loss: 0.2743 - val_acc: 0.9102\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.96484\n",
            "Epoch 159/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3260 - acc: 0.8768 - val_loss: 0.1710 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.96484\n",
            "Epoch 160/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2914 - acc: 0.9037 - val_loss: 0.1916 - val_acc: 0.9254\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.96484\n",
            "Epoch 161/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3438 - acc: 0.8735 - val_loss: 0.1944 - val_acc: 0.9315\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.96484\n",
            "Epoch 162/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2921 - acc: 0.9087 - val_loss: 0.1841 - val_acc: 0.9336\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.96484\n",
            "Epoch 163/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2497 - acc: 0.9106 - val_loss: 0.2042 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.96484\n",
            "Epoch 164/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.3010 - acc: 0.9026 - val_loss: 0.1967 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.96484\n",
            "Epoch 165/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2564 - acc: 0.9097 - val_loss: 0.1756 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.96484\n",
            "Epoch 166/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2886 - acc: 0.8979 - val_loss: 0.1377 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.96484\n",
            "Epoch 167/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.4180 - acc: 0.8649 - val_loss: 0.2645 - val_acc: 0.9154\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.96484\n",
            "Epoch 168/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.3936 - acc: 0.8474 - val_loss: 0.1787 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.96484\n",
            "Epoch 169/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2877 - acc: 0.9078 - val_loss: 0.1539 - val_acc: 0.9492\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.96484\n",
            "Epoch 170/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.3189 - acc: 0.8977 - val_loss: 0.1749 - val_acc: 0.9336\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.96484\n",
            "Epoch 171/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2615 - acc: 0.9071 - val_loss: 0.1697 - val_acc: 0.9414\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.96484\n",
            "Epoch 172/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2558 - acc: 0.9124 - val_loss: 0.1555 - val_acc: 0.9414\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.96484\n",
            "Epoch 173/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2097 - acc: 0.9189 - val_loss: 0.1748 - val_acc: 0.9303\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.96484\n",
            "Epoch 174/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2851 - acc: 0.8954 - val_loss: 0.1749 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.96484\n",
            "Epoch 175/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.2360 - acc: 0.9124 - val_loss: 0.1786 - val_acc: 0.9492\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.96484\n",
            "Epoch 176/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2074 - acc: 0.9329 - val_loss: 0.1239 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.96484\n",
            "Epoch 177/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2692 - acc: 0.9000 - val_loss: 0.1056 - val_acc: 0.9648\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.96484\n",
            "Epoch 178/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2797 - acc: 0.9055 - val_loss: 0.2129 - val_acc: 0.9353\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.96484\n",
            "Epoch 179/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3188 - acc: 0.8924 - val_loss: 0.1894 - val_acc: 0.9403\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.96484\n",
            "Epoch 180/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.2492 - acc: 0.9106 - val_loss: 0.1283 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.96484\n",
            "Epoch 181/200\n",
            "17/17 [==============================] - 28s 2s/step - loss: 0.2428 - acc: 0.9198 - val_loss: 0.1386 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.96484\n",
            "Epoch 182/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.2219 - acc: 0.9329 - val_loss: 0.1219 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.96484\n",
            "Epoch 183/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.2345 - acc: 0.9292 - val_loss: 0.1122 - val_acc: 0.9648\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.96484\n",
            "Epoch 184/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2648 - acc: 0.9163 - val_loss: 0.1486 - val_acc: 0.9552\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.96484\n",
            "Epoch 185/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2993 - acc: 0.9041 - val_loss: 0.1061 - val_acc: 0.9795\n",
            "\n",
            "Epoch 00185: val_acc improved from 0.96484 to 0.97945, saving model to /content/model.h5f\n",
            "Epoch 186/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2616 - acc: 0.9071 - val_loss: 0.2028 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.97945\n",
            "Epoch 187/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3571 - acc: 0.8808 - val_loss: 0.1060 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.97945\n",
            "Epoch 188/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.3538 - acc: 0.8815 - val_loss: 0.1661 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.97945\n",
            "Epoch 189/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2976 - acc: 0.8951 - val_loss: 0.1811 - val_acc: 0.9414\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.97945\n",
            "Epoch 190/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2587 - acc: 0.9073 - val_loss: 0.1303 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.97945\n",
            "Epoch 191/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2681 - acc: 0.9016 - val_loss: 0.1785 - val_acc: 0.9502\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.97945\n",
            "Epoch 192/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.3056 - acc: 0.8914 - val_loss: 0.1616 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.97945\n",
            "Epoch 193/200\n",
            "17/17 [==============================] - 33s 2s/step - loss: 0.2985 - acc: 0.8942 - val_loss: 0.1509 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.97945\n",
            "Epoch 194/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2418 - acc: 0.9145 - val_loss: 0.1492 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.97945\n",
            "Epoch 195/200\n",
            "17/17 [==============================] - 30s 2s/step - loss: 0.2596 - acc: 0.9007 - val_loss: 0.0940 - val_acc: 0.9805\n",
            "\n",
            "Epoch 00195: val_acc improved from 0.97945 to 0.98047, saving model to /content/model.h5f\n",
            "Epoch 196/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2590 - acc: 0.9055 - val_loss: 0.1057 - val_acc: 0.9552\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.98047\n",
            "Epoch 197/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2320 - acc: 0.9189 - val_loss: 0.1411 - val_acc: 0.9602\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.98047\n",
            "Epoch 198/200\n",
            "17/17 [==============================] - 31s 2s/step - loss: 0.2414 - acc: 0.9180 - val_loss: 0.1204 - val_acc: 0.9648\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.98047\n",
            "Epoch 199/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2286 - acc: 0.9265 - val_loss: 0.1595 - val_acc: 0.9414\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.98047\n",
            "Epoch 200/200\n",
            "17/17 [==============================] - 32s 2s/step - loss: 0.2382 - acc: 0.9301 - val_loss: 0.1303 - val_acc: 0.9492\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.98047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d1b6d1a09bcd94182216a4a4d2c77c77",
          "grade": false,
          "grade_id": "cell-307e87f69de91563",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "5Nz_C7uRMcKV",
        "colab_type": "text"
      },
      "source": [
        "# 2D - Quantitative Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "6640f3ceec388351da91e1594dff1879",
          "grade": true,
          "grade_id": "2d",
          "locked": false,
          "points": 8,
          "schema_version": 1,
          "solution": true
        },
        "id": "_GTZdjE8McKW",
        "colab_type": "code",
        "outputId": "40b1d679-10e7-409f-8767-28cf2192ba3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.plot(epochs, acc, 'red', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Training and validation loss')\n",
        "plt.plot(epochs, loss, 'red', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPQiCoAmJ4h4EQdCERHEPgiBoQv4Bmrzqx8fVQeoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLSSTe5IkSQvJ5J4kSdJCMrknSZK0kF8nN5pd+JMDaQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d382825e8158096ba3dc1fd8c40f89d1",
          "grade": false,
          "grade_id": "cell-0570698e87cb2784",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "gbWmfSLhMcKd",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "We can clearly see the difference, CNN architecture achieved 98% accuracy as compared to Fully Connected Feed Forward NN architecture which was able to achieve accuracy only up to 45%. <br>\n",
        "\n",
        "This shows us the importance of different NN architectures for different problems."
      ]
    }
  ]
}